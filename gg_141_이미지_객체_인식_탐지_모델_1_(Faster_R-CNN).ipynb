{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"gg_141_이미지_객체_인식_탐지_모델_1_(Faster_R-CNN).ipynb","provenance":[{"file_id":"https://github.com/Pseudo-Lab/Tutorial-Book/blob/master/book/chapters/object-detection/Ch5-Faster-R-CNN.ipynb","timestamp":1632960246251}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"524d149791d04509b52c0d2d0e5fb203":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1d41cef7e0304212b5db7ee357911648","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_32c1cf8f061f46cdb0421ee0cca62b46","IPY_MODEL_386eb588356e4f2ea357341f96de5116","IPY_MODEL_acf5261a079c400a8efc6ad26942a25d"]}},"1d41cef7e0304212b5db7ee357911648":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"32c1cf8f061f46cdb0421ee0cca62b46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_16744b5ca9d142a4beeac0ca0963488d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d8d407e1bf4644e5bb398516770c5aa3"}},"386eb588356e4f2ea357341f96de5116":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ec22574ded0942bf9b508bffdb81e9e2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":167502836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":167502836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bd8dac8c91ed4ccfa895235d3256c33c"}},"acf5261a079c400a8efc6ad26942a25d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6dc6e1a8da4a48d982c766b9329486a7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 160M/160M [00:02&lt;00:00, 83.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0617b633f1d94d6284d737f3988cfcfe"}},"16744b5ca9d142a4beeac0ca0963488d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d8d407e1bf4644e5bb398516770c5aa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec22574ded0942bf9b508bffdb81e9e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bd8dac8c91ed4ccfa895235d3256c33c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6dc6e1a8da4a48d982c766b9329486a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0617b633f1d94d6284d737f3988cfcfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WVdkYTL5wljD","executionInfo":{"status":"ok","timestamp":1636787580965,"user_tz":-540,"elapsed":26437,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}},"outputId":"7521c167-6109-49c4-9154-ff954f1ab6c5"},"source":["import torch\n","\n","if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla K80\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uz3fhZcEssjD","executionInfo":{"status":"ok","timestamp":1636787599864,"user_tz":-540,"elapsed":18917,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}},"outputId":"757f2bfd-206c-410e-9f52-1ad95c1ad96b"},"source":["!git clone https://github.com/Pseudo-Lab/Tutorial-Book-Utils\n","!python Tutorial-Book-Utils/PL_data_loader.py --data FaceMaskDetection\n","!unzip -q Face\\ Mask\\ Detection.zip"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'Tutorial-Book-Utils'...\n","remote: Enumerating objects: 30, done.\u001b[K\n","remote: Counting objects: 100% (30/30), done.\u001b[K\n","remote: Compressing objects: 100% (24/24), done.\u001b[K\n","remote: Total 30 (delta 9), reused 18 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (30/30), done.\n","Face Mask Detection.zip is done!\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jG9gk630gYqI","executionInfo":{"status":"ok","timestamp":1636787599865,"user_tz":-540,"elapsed":40,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}},"outputId":"9b8b5145-bec9-4505-b2a2-b00fbb3a7a11"},"source":["import os\n","import random\n","import numpy as np\n","import shutil\n","\n","print(len(os.listdir('annotations')))\n","print(len(os.listdir('images')))\n","\n","!mkdir test_images\n","!mkdir test_annotations\n","\n","\n","random.seed(1234)\n","idx = random.sample(range(853), 170)\n","\n","for img in np.array(sorted(os.listdir('images')))[idx]:\n","    shutil.move('images/'+img, 'test_images/'+img)\n","\n","for annot in np.array(sorted(os.listdir('annotations')))[idx]:\n","    shutil.move('annotations/'+annot, 'test_annotations/'+annot)\n","\n","print(len(os.listdir('annotations')))\n","print(len(os.listdir('images')))\n","print(len(os.listdir('test_annotations')))\n","print(len(os.listdir('test_images')))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["853\n","853\n","683\n","683\n","170\n","170\n"]}]},{"cell_type":"code","metadata":{"id":"DwpmvwQlx6mS","executionInfo":{"status":"ok","timestamp":1636787690309,"user_tz":-540,"elapsed":492,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}}},"source":["import os\n","import numpy as np\n","import matplotlib.patches as patches\n","import matplotlib.pyplot as plt\n","from bs4 import BeautifulSoup\n","from PIL import Image\n","import torchvision\n","from torchvision import transforms, datasets, models\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import time"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"4t0N0EXQzg5T","executionInfo":{"status":"ok","timestamp":1636787710805,"user_tz":-540,"elapsed":525,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}}},"source":["def generate_box(obj):\n","    \n","    xmin = float(obj.find('xmin').text)\n","    ymin = float(obj.find('ymin').text)\n","    xmax = float(obj.find('xmax').text)\n","    ymax = float(obj.find('ymax').text)\n","    \n","    return [xmin, ymin, xmax, ymax]\n","\n","adjust_label = 1\n","\n","def generate_label(obj):\n","\n","    if obj.find('name').text == \"with_mask\":\n","\n","        return 1 + adjust_label\n","\n","    elif obj.find('name').text == \"mask_weared_incorrect\":\n","\n","        return 2 + adjust_label\n","\n","    return 0 + adjust_label\n","\n","def generate_target(file): \n","    with open(file) as f:\n","        data = f.read()\n","        soup = BeautifulSoup(data, \"html.parser\")\n","        objects = soup.find_all(\"object\")\n","\n","        num_objs = len(objects)\n","\n","        boxes = []\n","        labels = []\n","        for i in objects:\n","            boxes.append(generate_box(i))\n","            labels.append(generate_label(i))\n","\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32) \n","        labels = torch.as_tensor(labels, dtype=torch.int64) \n","        \n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        \n","        return target\n","\n","def plot_image_from_output(img, annotation):\n","    \n","    img = img.cpu().permute(1,2,0)\n","    \n","    fig,ax = plt.subplots(1)\n","    ax.imshow(img)\n","    \n","    for idx in range(len(annotation[\"boxes\"])):\n","        xmin, ymin, xmax, ymax = annotation[\"boxes\"][idx]\n","\n","        if annotation['labels'][idx] == 1 :\n","            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='r',facecolor='none')\n","        \n","        elif annotation['labels'][idx] == 2 :\n","            \n","            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='g',facecolor='none')\n","            \n","        else :\n","        \n","            rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=1,edgecolor='orange',facecolor='none')\n","\n","        ax.add_patch(rect)\n","\n","    plt.show()"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtZuyC59veOa","executionInfo":{"status":"ok","timestamp":1636787764333,"user_tz":-540,"elapsed":459,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}}},"source":["class MaskDataset(object):\n","    def __init__(self, transforms, path):\n","        '''\n","        path: path to train folder or test folder\n","        '''\n","        # transform module과 img path 경로를 정의\n","        self.transforms = transforms\n","        self.path = path\n","        self.imgs = list(sorted(os.listdir(self.path)))\n","\n","\n","    def __getitem__(self, idx): #special method\n","        # load images ad masks\n","        file_image = self.imgs[idx]\n","        file_label = self.imgs[idx][:-3] + 'xml'\n","        img_path = os.path.join(self.path, file_image)\n","        \n","        if 'test' in self.path:\n","            label_path = os.path.join(\"test_annotations/\", file_label)\n","        else:\n","            label_path = os.path.join(\"annotations/\", file_label)\n","\n","        img = Image.open(img_path).convert(\"RGB\")\n","        #Generate Label\n","        target = generate_target(label_path)\n","        \n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","\n","        return img, target\n","\n","    def __len__(self): \n","        return len(self.imgs)\n","\n","data_transform = transforms.Compose([  # transforms.Compose : list 내의 작업을 연달아 할 수 있게 호출하는 클래스\n","        transforms.ToTensor() # ToTensor : numpy 이미지에서 torch 이미지로 변경\n","    ])\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","dataset = MaskDataset(data_transform, 'images/')\n","test_dataset = MaskDataset(data_transform, 'test_images/')\n","\n","data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\n","test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6anTbFtQLCh","executionInfo":{"status":"ok","timestamp":1636787769015,"user_tz":-540,"elapsed":3,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}}},"source":["def get_model_instance_segmentation(num_classes):\n","  \n","    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","    return model"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["524d149791d04509b52c0d2d0e5fb203","1d41cef7e0304212b5db7ee357911648","32c1cf8f061f46cdb0421ee0cca62b46","386eb588356e4f2ea357341f96de5116","acf5261a079c400a8efc6ad26942a25d","16744b5ca9d142a4beeac0ca0963488d","d8d407e1bf4644e5bb398516770c5aa3","ec22574ded0942bf9b508bffdb81e9e2","bd8dac8c91ed4ccfa895235d3256c33c","6dc6e1a8da4a48d982c766b9329486a7","0617b633f1d94d6284d737f3988cfcfe"]},"id":"bUOpks69QLE_","executionInfo":{"status":"ok","timestamp":1636787792190,"user_tz":-540,"elapsed":18738,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}},"outputId":"56b10371-3531-43fc-e8f2-832b245e8246"},"source":["model = get_model_instance_segmentation(4)\n","\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n","model.to(device)"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"524d149791d04509b52c0d2d0e5fb203","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0.00/160M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048, eps=0.0)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=4, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=16, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Zws-xQdwQLMZ","executionInfo":{"status":"ok","timestamp":1636787899477,"user_tz":-540,"elapsed":473,"user":{"displayName":"Yongjin Jeong","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03658406798560557048"}}},"source":["num_epochs = 10\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                                momentum=0.9, weight_decay=0.0005)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWFbYKAWtha2","outputId":"2a3cb134-c210-4048-b17e-eec7c02a755d"},"source":["print('----------------------train start--------------------------')\n","for epoch in range(num_epochs):\n","    start = time.time()\n","    model.train()\n","    i = 0    \n","    epoch_loss = 0\n","    for imgs, annotations in data_loader:\n","        i += 1\n","        imgs = list(img.to(device) for img in imgs)\n","        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n","        loss_dict = model(imgs, annotations) \n","        losses = sum(loss for loss in loss_dict.values())        \n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step() \n","        epoch_loss += losses\n","    print(f'epoch : {epoch+1}, Loss : {epoch_loss}, time : {time.time() - start}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------train start--------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]},{"output_type":"stream","name":"stdout","text":["epoch : 1, Loss : 78.10614776611328, time : 739.8335559368134\n","epoch : 2, Loss : 49.56209945678711, time : 735.7399907112122\n"]}]},{"cell_type":"code","metadata":{"id":"vzCqYLX7R5Hv"},"source":["torch.save(model.state_dict(),f'model_{num_epochs}.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g08rExoWCxh2"},"source":["model.load_state_dict(torch.load(f'model_{num_epochs}.pt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ywqv5--JlFbd"},"source":["def make_prediction(model, img, threshold):\n","    model.eval()\n","    preds = model(img)\n","    for id in range(len(preds)) :\n","        idx_list = []\n","\n","        for idx, score in enumerate(preds[id]['scores']) :\n","            if score > threshold : \n","                idx_list.append(idx)\n","\n","        preds[id]['boxes'] = preds[id]['boxes'][idx_list]\n","        preds[id]['labels'] = preds[id]['labels'][idx_list]\n","        preds[id]['scores'] = preds[id]['scores'][idx_list]\n","\n","    return preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x4k8KTVk6BnG"},"source":["with torch.no_grad(): \n","    # 테스트셋 배치사이즈= 2\n","    for imgs, annotations in test_data_loader:\n","        imgs = list(img.to(device) for img in imgs)\n","\n","        pred = make_prediction(model, imgs, 0.5)\n","        print(pred)\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"njyv4Gu-6CDK"},"source":["_idx = 1\n","print(\"Target : \", annotations[_idx]['labels'])\n","plot_image_from_output(imgs[_idx], annotations[_idx])\n","print(\"Prediction : \", pred[_idx]['labels'])\n","plot_image_from_output(imgs[_idx], pred[_idx])"],"execution_count":null,"outputs":[]}]}